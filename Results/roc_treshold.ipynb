{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2ad2dbe9-8395-4a0e-ade9-1d75f639a471",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import gc\n",
    "import torch\n",
    "from torch.nn.functional import softmax\n",
    "from sklearn.metrics import precision_score, recall_score, confusion_matrix\n",
    "from sklearn.metrics import auc\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c57b51ed-43c4-46ad-a5c4-5469a4494829",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Funcion para hallar la especificidad\n",
    "def specificity_score(y_true, y_pred):\n",
    "    matrix = confusion_matrix(y_true, y_pred)\n",
    "    if matrix.size == 1:\n",
    "        tn = matrix[0][0]\n",
    "        fp = 0\n",
    "    else:\n",
    "        tn, fp, fn, tp = matrix.ravel()\n",
    "    specificity = tn / (tn+fp)\n",
    "    return specificity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf3d5400-dd6b-46d9-a4ad-c5a6ec946fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_metrics_(model, test_loader):\n",
    "    # Inicializamos un diccionario para almacenar las métricas\n",
    "    metric_results = {}\n",
    "\n",
    "    # Iteramos sobre el conjunto de test\n",
    "    for i, (images, masks) in tqdm(enumerate(test_loader), total=len(test_loader)):\n",
    "\n",
    "        # Permuta las dimensiones de las imágenes a (N, C, H, W)\n",
    "        images = images.permute(0, 3, 1, 2)\n",
    "        images, masks = images.to(device, dtype=torch.float), masks.to(device,  dtype=torch.long)\n",
    "\n",
    "        outputs = model(images)\n",
    "        outputs = softmax(outputs, dim=1)\n",
    "\n",
    "        # Convertimos las predicciones y las etiquetas a arrays de numpy\n",
    "        all_preds = outputs.detach().cpu().numpy()\n",
    "        all_labels = masks.cpu().numpy()\n",
    "\n",
    "        # Iteramos sobre los umbrales y las clases\n",
    "        for threshold in [round(x * 0.1, 1) for x in range(0, 11)]:\n",
    "            for class_index in range(5):\n",
    "                # Binarizamos las salidas\n",
    "                preds = (all_preds[:, class_index, :, :] > threshold).reshape(-1)\n",
    "                true = (all_labels == class_index).reshape(-1)\n",
    "\n",
    "                # Calculamos las métricas\n",
    "                precision = precision_score(true, preds)\n",
    "                recall = recall_score(true, preds)  # Sensibilidad\n",
    "                specificity = specificity_score(true, preds)\n",
    "\n",
    "                # Almacenamos las métricas en el diccionario\n",
    "                if class_index not in metric_results:\n",
    "                    metric_results[class_index] = {'thresholds': [], 'specificity': [], 'recall': [], 'precision': []}\n",
    "                \n",
    "                # Añadimos los valores de las métricas\n",
    "                metric_results[class_index]['thresholds'].append(round(threshold, 2))\n",
    "                metric_results[class_index]['specificity'].append(round(specificity, 5))\n",
    "                metric_results[class_index]['recall'].append(round(recall, 5))\n",
    "                metric_results[class_index]['precision'].append(round(precision, 5))\n",
    "\n",
    "        del images, masks, outputs, all_preds, all_labels\n",
    "        gc.collect()\n",
    "\n",
    "    return metric_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9f59468b-e4b9-457c-a7eb-5a7dc0731677",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "incomplete input (2932779966.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[1], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    def mean_metrics():\u001b[0m\n\u001b[0m                       ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m incomplete input\n"
     ]
    }
   ],
   "source": [
    "def mean_metrics(metric_results):\n",
    "    averaged_metrics = {}\n",
    "\n",
    "    for class_index in metric_results.keys():\n",
    "        averaged_metrics[class_index] = {'thresholds': [], 'specificity': [], 'recall': [], 'precision': []}\n",
    "\n",
    "        # Obtenemos las listas de métricas para esta clase\n",
    "        thresholds = metric_results[class_index]['thresholds']\n",
    "        specificity = metric_results[class_index]['specificity']\n",
    "        recall = metric_results[class_index]['recall']\n",
    "        precision = metric_results[class_index]['precision']\n",
    "\n",
    "        # Calculamos la media de las métricas para cada umbral\n",
    "        for threshold in set(thresholds):\n",
    "            indices = [i for i, x in enumerate(thresholds) if x == threshold]\n",
    "\n",
    "            averaged_metrics[class_index]['thresholds'].append(threshold)\n",
    "            averaged_metrics[class_index]['specificity'].append(np.mean([specificity[i] for i in indices]))\n",
    "            averaged_metrics[class_index]['recall'].append(np.mean([recall[i] for i in indices]))\n",
    "            averaged_metrics[class_index]['precision'].append(np.mean([precision[i] for i in indices]))\n",
    "    \n",
    "    return averaged_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1c45a92f-80ec-4ba8-ab77-a75585e3ca34",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def print_metrics(averaged_metrics, classes):\n",
    "    \n",
    "    for class_index in averaged_metrics.keys():\n",
    "        # Obtenemos las listas de métricas para esta clase\n",
    "        thresholds = averaged_metrics[class_index]['thresholds']\n",
    "        specificity = averaged_metrics[class_index]['specificity']\n",
    "        recall = averaged_metrics[class_index]['recall']\n",
    "        precision = averaged_metrics[class_index]['precision']\n",
    "\n",
    "        # Creamos una lista de tuplas, donde cada tupla contiene el umbral y las métricas correspondientes\n",
    "        metrics = list(zip(thresholds, specificity, recall, precision))\n",
    "\n",
    "        # Ordenamos la lista de tuplas por el umbral\n",
    "        metrics.sort()\n",
    "\n",
    "        # Desempaquetamos la lista de tuplas ordenada de nuevo en las listas de métricas\n",
    "        thresholds, specificity, recall, precision = zip(*metrics)\n",
    "\n",
    "        # Actualizamos las listas de métricas en el diccionario\n",
    "        averaged_metrics[class_index]['thresholds'] = list(thresholds)\n",
    "        averaged_metrics[class_index]['specificity'] = list(specificity)\n",
    "        averaged_metrics[class_index]['recall'] = list(recall)\n",
    "        averaged_metrics[class_index]['precision'] = list(precision)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Crear DataFrames para cada clase\n",
    "    dataframes = {}\n",
    "    for class_index, metrics in averaged_metrics.items():\n",
    "        data = {\n",
    "            'Threshold': metrics['thresholds'],\n",
    "            'Sensibilidad': metrics['recall'],\n",
    "            'Especificidad': metrics['specificity'],\n",
    "            'Precision': metrics['precision']\n",
    "        }\n",
    "\n",
    "        df = pd.DataFrame(data)\n",
    "        df = df.round(5) \n",
    "        dataframes[classes[class_index]] = df\n",
    "\n",
    "    # Acceder a los DataFrames individuales\n",
    "    for class_name, df in dataframes.items():\n",
    "        print(f\"\\Resultados para la clase {class_name}:\\n\")\n",
    "        print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "70e71eef-1d2a-4b65-a78a-bd98f8d71270",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_roc_curve(metric_results, classes):\n",
    "    \n",
    "    # Accede a los resultados almacenados\n",
    "    for class_index, metrics in metric_results.items():\n",
    "        thresholds = metrics['thresholds']\n",
    "        specificity_values = metrics['specificity']\n",
    "        recall_values = metrics['recall']\n",
    "        precision_values = metrics['precision']\n",
    "\n",
    "        # Calcula la tasa de falsos positivos (1 - especificidad)\n",
    "        fpr = [1 - spec for spec in specificity_values]\n",
    "\n",
    "        # Calcula el área bajo la curva ROC\n",
    "        roc_auc = auc(fpr, recall_values)\n",
    "\n",
    "        plt.figure()\n",
    "        plt.plot(fpr, recall_values, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')\n",
    "        plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "        plt.xlabel('False Positive Rate')\n",
    "        plt.ylabel('True Positive Rate')\n",
    "        plt.title('ROC Curve - Class ' + str(class_index) + \" ~ \" + classes[class_index])\n",
    "        plt.legend(loc='lower right')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03a0f8a5-abac-4a44-ab35-4f94fa728222",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
