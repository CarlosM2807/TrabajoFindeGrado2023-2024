{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2ad2dbe9-8395-4a0e-ade9-1d75f639a471",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import gc\n",
    "import torch\n",
    "from torch.nn.functional import softmax\n",
    "from sklearn.metrics import precision_score, recall_score, confusion_matrix\n",
    "from sklearn.metrics import auc\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c57b51ed-43c4-46ad-a5c4-5469a4494829",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Funcion para hallar la especificidad\n",
    "# Argumentos:\n",
    "#      - y_true: vector que contiene los valores de las etiquetas verdaderas\n",
    "#      - y_pred: vector que contiene los valores de las etiquetas predichas\n",
    "#\n",
    "\n",
    "def specificity_score(y_true, y_pred):\n",
    "    \n",
    "    # Calculamos la matriz de confusion\n",
    "    matrix = confusion_matrix(y_true, y_pred)\n",
    "    \n",
    "    # Si predicho y verdaderos son iguales\n",
    "    if matrix.size == 1:\n",
    "        tn = matrix[0][0]\n",
    "        fp = 0\n",
    "    else:\n",
    "        tn = matrix[0][0]\n",
    "        fp = matrix[0][1]\n",
    "        fn = matrix[1][0]\n",
    "        tp = matrix[1][1]\n",
    "    \n",
    "    # Hallamos la especificidad\n",
    "    specificity = tn / (tn+fp)\n",
    "    return specificity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bf3d5400-dd6b-46d9-a4ad-c5a6ec946fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funcion halla las métricas de especificidad, precision y sensibilidad, para un lote\n",
    "# Argumentos:\n",
    "#      - model: contiene el modelo preentrenado\n",
    "#      - test_loader: lote de imagenes de prueba\n",
    "#\n",
    "\n",
    "def build_metrics_(model, test_loader):\n",
    "    \n",
    "    # Inicializamos un diccionario para almacenar las métricas\n",
    "    metric_results = {}\n",
    "\n",
    "    # Iteramos sobre el conjunto de test\n",
    "    for i, (images, masks) in tqdm(enumerate(test_loader), total=len(test_loader)):\n",
    "\n",
    "        # Permuta las dimensiones de las imágenes a (N, C, H, W)\n",
    "        images = images.permute(0, 3, 1, 2)\n",
    "        \n",
    "        # Movemos al device las imagenes y las Ground Truth\n",
    "        images = images.to(device, dtype=torch.float)\n",
    "        masks = masks.to(device, dtype=torch.long)\n",
    "        \n",
    "        # El modelo realiza la inferencia para las imagenes del lote\n",
    "        outputs = model(images)\n",
    "        \n",
    "        # Aplicamos la funcion softmax para obtener las probabilidad\n",
    "        outputs = softmax(outputs, dim=1)\n",
    "\n",
    "        # Convertimos las predicciones y las etiquetas a arrays de numpy\n",
    "        all_preds = outputs.detach().cpu().numpy()\n",
    "        all_labels = masks.cpu().numpy()\n",
    "\n",
    "        # Iteramos sobre los umbrales y las clases\n",
    "        for threshold in [round(x * 0.1, 1) for x in range(0, 11)]:\n",
    "            \n",
    "            for idx_class in range(5):\n",
    "                \n",
    "                # Binarizamos las salidas\n",
    "                preds = (all_preds[:, idx_class, :, :] > threshold).reshape(-1)\n",
    "                \n",
    "                # Creamos un vector con las etiquetas reales\n",
    "                true = (all_labels == idx_class).reshape(-1)\n",
    "\n",
    "                # Calculamos las métricas\n",
    "                precision = precision_score(true, preds)\n",
    "                recall = recall_score(true, preds)  \n",
    "                specificity = specificity_score(true, preds)\n",
    "\n",
    "                # Añadimos un diccionario para cada clase\n",
    "                if idx_class not in metric_results:\n",
    "                    metric_results[idx_class] = {'thresholds': [], 'specificity': [], 'recall': [], 'precision': []}\n",
    "                \n",
    "                # Añadimos los valores de las métricas\n",
    "                metric_results[idx_class]['thresholds'].append(round(threshold, 2))\n",
    "                metric_results[idx_class]['specificity'].append(round(specificity, 5))\n",
    "                metric_results[idx_class]['recall'].append(round(recall, 5))\n",
    "                metric_results[idx_class]['precision'].append(round(precision, 5))\n",
    "\n",
    "        # Limpiamos \n",
    "        del images, masks, outputs, all_preds, all_labels\n",
    "        gc.collect()\n",
    "\n",
    "    return metric_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9f59468b-e4b9-457c-a7eb-5a7dc0731677",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Halla las medias de las metricas para todos los lotes, generando asi la media global\n",
    "# Argumentos:\n",
    "#      - metric_results: contiene las métricas para todos los lotes\n",
    "\n",
    "def mean_metrics(metric_results):\n",
    "    \n",
    "    # Diccionario que tiene las metricas medias para cada clase\n",
    "    mean_metrics = {}\n",
    "\n",
    "    for idx_class in metric_results.keys():\n",
    "\n",
    "        # Creamos un array de diccionarios, donde cada uno tiene las metricas para cada clase\n",
    "        mean_metrics[idx_class] = {'thresholds': [], 'specificity': [], 'recall': [], 'precision': []}\n",
    "\n",
    "        # Obtenemos las listas de métricas para esta clase\n",
    "        thresholds = metric_results[idx_class]['thresholds']\n",
    "        specificity = metric_results[idx_class]['specificity']\n",
    "        recall = metric_results[idx_class]['recall']\n",
    "        precision = metric_results[idx_class]['precision']\n",
    "\n",
    "        # Calculamos la media de las métricas para cada umbral, así agrupamos las metricas por umbral\n",
    "        for threshold in set(thresholds):\n",
    "            \n",
    "            # Creamos una lista vacía para almacenar los índices\n",
    "            indices = []\n",
    "\n",
    "            # Recorremos thresholds\n",
    "            for i in range(len(thresholds)):\n",
    "                \n",
    "                # Obtenemos el elemento en la posición 'i'\n",
    "                x = thresholds[i]\n",
    "    \n",
    "                if x == threshold:\n",
    "                    indices.append(i)\n",
    "                \n",
    "            # Conseguimos que indices tenga la metricas agrupada por umbral\n",
    "            mean_metrics[idx_class]['thresholds'].append(threshold)\n",
    "            \n",
    "            # Hallamos las medias para las 3 metricas\n",
    "            mean_metrics[idx_class]['specificity'].append(np.mean([specificity[i] for i in indices]))\n",
    "            mean_metrics[idx_class]['recall'].append(np.mean([recall[i] for i in indices]))\n",
    "            mean_metrics[idx_class]['precision'].append(np.mean([precision[i] for i in indices]))\n",
    "    \n",
    "    return mean_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1c45a92f-80ec-4ba8-ab77-a75585e3ca34",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Printea las métricas globales\n",
    "# Argumentos\n",
    "#      - averaged_metrics: contiene las métricas globales\n",
    "#      - classes: lista con los nombres de las clases\n",
    "\n",
    "def print_metrics(mean_metrics, classes):\n",
    "    \n",
    "    for idx_class in mean_metrics.keys():\n",
    "        \n",
    "        # Obtenemos las listas de métricas para esta clase\n",
    "        thresholds = mean_metrics[idx_class]['thresholds']\n",
    "        specificity = mean_metrics[idx_class]['specificity']\n",
    "        recall = mean_metrics[idx_class]['recall']\n",
    "        precision = mean_metrics[idx_class]['precision']\n",
    "\n",
    "        # Creamos una lista de tuplas, donde cada tupla contiene el umbral y las métricas correspondientes\n",
    "        metrics = list(zip(thresholds, specificity, recall, precision))\n",
    "\n",
    "        # Ordenamos la lista de tuplas por el umbral\n",
    "        metrics.sort()\n",
    "\n",
    "        # Desempaquetamos la lista de tuplas ordenada de nuevo en las listas de métricas\n",
    "        thresholds, specificity, recall, precision = zip(*metrics)\n",
    "\n",
    "        # Actualizamos las listas de métricas en el diccionario\n",
    "        mean_metrics[idx_class]['thresholds'] = list(thresholds)\n",
    "        mean_metrics[idx_class]['specificity'] = list(specificity)\n",
    "        mean_metrics[idx_class]['recall'] = list(recall)\n",
    "        mean_metrics[idx_class]['precision'] = list(precision)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Crear DataFrames para cada clase\n",
    "    all_dataframes = {}\n",
    "\n",
    "    # Generamos un DataFrame para cada clase\n",
    "    for idx_class, metrics in mean_metrics.items():\n",
    "        data = {\n",
    "            'Threshold': metrics['thresholds'],\n",
    "            'Sensibilidad': metrics['recall'],\n",
    "            'Especificidad': metrics['specificity'],\n",
    "            'Precision': metrics['precision']\n",
    "        }\n",
    "\n",
    "        # Creamos el DataFrame\n",
    "        df = pd.DataFrame(data)\n",
    "\n",
    "        # Redondeamos a 5 decimales\n",
    "        df = df.round(5) \n",
    "\n",
    "        # Almacenamos el DataFrame en el diccionario\n",
    "        # key = nombre de la clase\n",
    "        # value = el diccionario\n",
    "        all_dataframes[classes[idx_class]] = df\n",
    "\n",
    "    # Acceder a los DataFrames individuales\n",
    "    for class_name, df in all_dataframes.items():\n",
    "\n",
    "        # Imprimimos las metricas\n",
    "        print(f\"\\Resultados para la clase {class_name}:\\n\")\n",
    "        print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "70e71eef-1d2a-4b65-a78a-bd98f8d71270",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dibuja las curvas ROC, una para cada clase\n",
    "# Argumentos\n",
    "#      - metric_results: contiene las métricas globales\n",
    "#      - classes: lista con los nombres de las clases\n",
    "\n",
    "def print_roc_curve(metric_results, classes):\n",
    "    \n",
    "    # Accede a los resultados almacenados\n",
    "    for idx_class, metrics in metric_results.items():\n",
    "\n",
    "        # Obtenemos las metricas d interes\n",
    "        thresholds = metrics['thresholds']\n",
    "        specificity_values = metrics['specificity']\n",
    "        recall_values = metrics['recall']\n",
    "\n",
    "        # Calculamos la tasa de falsos positivos (1 - especificidad)\n",
    "        fpr = [1 - spec for spec in specificity_values]\n",
    "\n",
    "        # Calculamos el área bajo la curva ROC\n",
    "        roc_auc = auc(fpr, recall_values)\n",
    "\n",
    "        plt.figure()\n",
    "\n",
    "        # Dibujamos la curva roc\n",
    "        plt.plot(fpr, recall_values, color='orange', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')\n",
    "        \n",
    "        # Definimos que los ejes, van de 0 a 1\n",
    "        plt.plot([0, 1], [0, 1], color='blue', lw=2, linestyle='-')\n",
    "\n",
    "        # Etiquetas\n",
    "        plt.xlabel('False Positive Rate')\n",
    "        plt.ylabel('True Positive Rate')\n",
    "        plt.title('ROC Curve - Class ' + str(idx_class) + \" ~ \" + classes[idx_class])\n",
    "\n",
    "        # Elegimos la posicion de la leyenda\n",
    "        plt.legend(loc='lower right')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03a0f8a5-abac-4a44-ab35-4f94fa728222",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
